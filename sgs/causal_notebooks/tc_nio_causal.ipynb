{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfe2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saranya/anaconda3/envs/bmap/lib/python3.8/site-packages/tigramite/models.py:29: UserWarning: [Errno 2] No such file or directory: '/home/saranya/anaconda3/envs/bmap/lib/python3.8/site-packages/tigramite/../versions.py'\n",
      "  warnings.warn(str(e))\n",
      "/home/saranya/anaconda3/envs/bmap/lib/python3.8/site-packages/tigramite/plotting.py:26: UserWarning: [Errno 2] No such file or directory: '/home/saranya/anaconda3/envs/bmap/lib/python3.8/site-packages/tigramite/../versions.py'\n",
      "  warnings.warn(str(e))\n",
      "/home/saranya/anaconda3/envs/bmap/lib/python3.8/site-packages/tigramite/independence_tests/gpdc.py:27: UserWarning: [Errno 2] No such file or directory: '/home/saranya/anaconda3/envs/bmap/lib/python3.8/site-packages/tigramite/independence_tests/../../versions.py'\n",
      "  warnings.warn(str(e))\n",
      "/home/saranya/anaconda3/envs/bmap/lib/python3.8/site-packages/tigramite/independence_tests/gpdc_torch.py:33: UserWarning: No module named 'torch'\n",
      "  warnings.warn(str(e))\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline     \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob,os\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.toymodels import structural_causal_processes as toys\n",
    "from tigramite.models import Models, Prediction\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr, GPDC, CMIknn, CMIsymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28d0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_links(tau_min, tau_max, parents, children):\n",
    "    \"\"\"\n",
    "    This function selects the causal links that will be tested by\n",
    "    PCMCI. The links are selected such that per each variable in\n",
    "    `children` all `parents` are stablished as causes, and no other\n",
    "    causal relationships exist.\n",
    "    \n",
    "    Assumes `parents` and `children` are disjoint sets, and that all\n",
    "    variables are included in the union of both sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tau_min : int\n",
    "        Minimum time lag to test. Note that zero-lags are undirected.\n",
    "    tau_max : int\n",
    "        Maximum time lag. Must be larger or equal to tau_min.\n",
    "    parents : set of int\n",
    "        List of variables that will be assigned as a parent link.\n",
    "        Assumed to be disjoint with children\n",
    "    children : set of int\n",
    "        List of variables that will be assigned a link from a parent.\n",
    "        Assumed to be disjoint with parents\n",
    "    Returns\n",
    "    -------\n",
    "    selected_links: dict\n",
    "        Dictionary of selected links for Tigramite\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    parents = set(parents)\n",
    "    children = set(children)\n",
    "\n",
    "    selected_links = dict()\n",
    "    # Set the default as all combinations of the selected variables\n",
    "    for var in [*children, *parents]:\n",
    "        if var in children:\n",
    "            # Children can be caused only by parents and by themselves\n",
    "            selected_links[var] = [\n",
    "                (parent, -lag)\n",
    "                for parent in parents\n",
    "                for lag in range(tau_min, tau_max + 1)\n",
    "            ]\n",
    "        else:\n",
    "            selected_links[var] = []\n",
    "\n",
    "    return selected_links\n",
    "\n",
    "def _process_dataset(path=None):\n",
    "    df1 = pd.read_csv(path,sep=',')\n",
    "    df1.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "    df1=df1.drop('a', axis=1)\n",
    "    df1=df1.drop('conv_rrate', axis=1)\n",
    "    df1=df1.drop('ls_rrate', axis=1)\n",
    "    df1=df1.drop('mn_conv_prate', axis=1)\n",
    "    df1=df1.drop('mn_ls_prate', axis=1)\n",
    "    df1=df1.drop('mn_tot_prate', axis=1)\n",
    "    df1=df1.drop('outconv_rrate', axis=1)\n",
    "    df1=df1.drop('outls_rrate', axis=1)\n",
    "    df1=df1.drop('outmn_conv_prate', axis=1)\n",
    "    df1=df1.drop('outmn_ls_prate', axis=1)\n",
    "    df1=df1.drop('outmn_tot_prate', axis=1)\n",
    "    df1=df1.drop('conv_ppt', axis=1)\n",
    "    df1=df1.drop('outconv_ppt', axis=1)\n",
    "    \n",
    "    TCname = path.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    #print(TCname)\n",
    "    for item in glob.glob('/media/saranya/DAEA17D6EA17ADAF/Data/Research_2022/jun22/targets/*tot_ppt_int*'):\n",
    "        if str(TCname) in item:\n",
    "            d1=pd.read_csv(item)\n",
    "            d1.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "            d1=d1.drop('a', axis=1)\n",
    "            dt1=pd.concat([d1,df1],axis=1, join='inner')\n",
    "        else:\n",
    "            continue\n",
    "    return dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a6a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=\"../../tigramite_timeseries/\"\n",
    "p2=\"../../targets/\"\n",
    "\n",
    "dt1=_process_dataset(glob.glob(p1+'*keila*')[0])\n",
    "dt2=_process_dataset(glob.glob(p1+'*viyaru*')[0])\n",
    "dt3=_process_dataset(glob.glob(p1+'*phailin*')[0])\n",
    "dt4=_process_dataset(glob.glob(p1+'*lehar*')[0])\n",
    "dt5=_process_dataset(glob.glob(p1+'*madi*')[0])\n",
    "dt6=_process_dataset(glob.glob(p1+'*nanauk*')[0])\n",
    "dt7=_process_dataset(glob.glob(p1+'*hudhud*')[0])\n",
    "dt8=_process_dataset(glob.glob(p1+'*nilofar*')[0])\n",
    "dt9=_process_dataset(glob.glob(p1+'*ashoba*')[0])\n",
    "dt10=_process_dataset(glob.glob(p1+'*chapala*')[0])\n",
    "dt11=_process_dataset(glob.glob(p1+'*megh*')[0])\n",
    "dt12=_process_dataset(glob.glob(p1+'*roanu*')[0])\n",
    "dt13=_process_dataset(glob.glob(p1+'*kyant*')[0])\n",
    "dt14=_process_dataset(glob.glob(p1+'*vardah*')[0])\n",
    "dt15=_process_dataset(glob.glob(p1+'*ockhi*')[0])\n",
    "dt16=_process_dataset(glob.glob(p1+'*luban*')[0])\n",
    "dt17=_process_dataset(glob.glob(p1+'*gaja*')[0])\n",
    "dt18=_process_dataset(glob.glob(p1+'*fani*')[0])\n",
    "dt19=_process_dataset(glob.glob(p1+'*vayu*')[0])\n",
    "dt20=_process_dataset(glob.glob(p1+'*kyaar*')[0])\n",
    "dt21=_process_dataset(glob.glob(p1+'*maha*')[0])\n",
    "dt22=_process_dataset(glob.glob(p1+'*bulbul*')[0])\n",
    "dt23=_process_dataset(glob.glob(p1+'*pawan*')[0])\n",
    "dt24=_process_dataset(glob.glob(p1+'*amphan*')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3f6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcnio1=dt1.values\n",
    "tcnio2=dt2.values\n",
    "tcnio3=dt3.values\n",
    "tcnio4=dt4.values\n",
    "tcnio5=dt5.values\n",
    "tcnio6=dt6.values\n",
    "tcnio7=dt7.values\n",
    "tcnio8=dt8.values\n",
    "tcnio9=dt9.values\n",
    "tcnio10=dt10.values\n",
    "tcnio11=dt11.values\n",
    "tcnio12=dt12.values\n",
    "tcnio13=dt13.values\n",
    "tcnio14=dt14.values\n",
    "tcnio15=dt15.values\n",
    "tcnio16=dt16.values\n",
    "tcnio17=dt17.values\n",
    "tcnio18=dt18.values\n",
    "tcnio19=dt19.values\n",
    "tcnio20=dt20.values\n",
    "tcnio21=dt21.values\n",
    "tcnio22=dt22.values\n",
    "tcnio23=dt23.values\n",
    "tcnio24=dt24.values\n",
    "\n",
    "ddnio={'cyclone_nio1':tcnio1,'cyclone_nio2':tcnio2,'cyclone_nio3':tcnio3,'cyclone_nio4':tcnio4,'cyclone_nio5':tcnio5,'cyclone_nio6':tcnio6,\n",
    "      'cyclone_nio7':tcnio7, 'cyclone_nio8':tcnio8,'cyclone_nio9':tcnio9,'cyclone_nio10':tcnio10,'cyclone_nio11':tcnio11,'cyclone_nio12':tcnio12,\n",
    "      'cyclone_nio13':tcnio13,'cyclone_nio14':tcnio14,'cyclone_nio15':tcnio15,'cyclone_nio16':tcnio16,'cyclone_nio17':tcnio17,'cyclone_nio18':tcnio18,\n",
    "      'cyclone_nio19':tcnio19,'cyclone_nio20':tcnio20,'cyclone_nio21':tcnio21,'cyclone_nio22':tcnio22,'cyclone_nio23':tcnio23,'cyclone_nio24':tcnio24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e243922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ddnio.keys())[0][11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bab049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    Tigramite and Linear Regression Pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self,data,pc_alpha,alpha_level,pc_type='run_pcstable' or 'pcmci',tau_min0=None,tau_max0=None,\n",
    "                 target='precip',var_name=None,seed=None,cond_ind_test=ParCorr()):\n",
    "        self.pc_alpha = pc_alpha\n",
    "        self.alpha_level = alpha_level\n",
    "        self.data = data\n",
    "        self.pc_type = pc_type\n",
    "        self.target = target\n",
    "        self.tau_min0 = tau_min0\n",
    "        self.tau_max0 = tau_max0\n",
    "        self.var_name = var_name\n",
    "        self.seed = seed\n",
    "        self.cond_ind_test = cond_ind_test\n",
    "        \n",
    "    #################################################################################\n",
    "    # Step 0: Split\n",
    "    #################################################################################\n",
    "    def splitdata(self,testindex=None,basin='WPAC'):\n",
    "        datae = self.data.copy()\n",
    "        traindata = {}\n",
    "        testdata = {}\n",
    "        validdata = {}\n",
    "        validindex,newtestindex = testindex[:int(len(testindex)/2)],testindex[int(len(testindex)/2):]\n",
    "        for obj in datae.keys():\n",
    "            if basin=='WPAC':\n",
    "                number = int(obj[7:])\n",
    "                if number in list(newtestindex):\n",
    "                    testdata['cyclone'+str(number)] = datae['cyclone'+str(number)]\n",
    "                elif number in list(validindex):\n",
    "                    validdata['cyclone'+str(number)] = datae['cyclone'+str(number)]\n",
    "                else:\n",
    "                    traindata['cyclone'+str(number)] = datae['cyclone'+str(number)]\n",
    "            elif basin=='NIO':\n",
    "                number = int(obj[11:])\n",
    "                if number in list(newtestindex):\n",
    "                    testdata['cyclone_nio'+str(number)] = datae['cyclone_nio'+str(number)]\n",
    "                elif number in list(validindex):\n",
    "                    validdata['cyclone_nio'+str(number)] = datae['cyclone_nio'+str(number)]\n",
    "                else:\n",
    "                    traindata['cyclone_nio'+str(number)] = datae['cyclone_nio'+str(number)]\n",
    "        return traindata,validdata,testdata\n",
    "    \n",
    "    #################################################################################\n",
    "    # Step 1: Tigramite\n",
    "    #################################################################################\n",
    "    def run_tigramite(self,lengthtrain=None):\n",
    "        #assert len(self.data)==lengthtrain,\"Wrong shape!\"\n",
    "        datae = self.data.copy()\n",
    "        dataframe =  pp.DataFrame(datae,analysis_mode ='multiple', var_names=var_names)\n",
    "        #################################################################################\n",
    "        # Sel links\n",
    "        #################################################################################\n",
    "        for member in dataframe.values.keys():\n",
    "            children = [0,1,2]\n",
    "            parents = np.arange(3,259)\n",
    "            sel_links = select_links(self.tau_min0, self.tau_max0, parents, children)\n",
    "        #################################################################################\n",
    "        # Run Tigramite\n",
    "        #################################################################################        \n",
    "        pcmci = PCMCI(dataframe = dataframe, cond_ind_test = self.cond_ind_test)\n",
    "        if self.pc_type=='run_pcstable':\n",
    "            results = pcmci.run_pcstable(selected_links=sel_links, tau_min=self.tau_min0, tau_max=self.tau_max0,\\\n",
    "                                          pc_alpha=self.pc_alpha)\n",
    "        elif self.pc_type=='pcmci':\n",
    "            results = pcmci.run_pcmci(selected_links=sel_links, tau_min=self.tau_min0, tau_max=self.tau_max0,\\\n",
    "                                      pc_alpha=self.pc_alpha)\n",
    "\n",
    "        pcmci.verbosity = 2\n",
    "        #################################################################################\n",
    "        # Test\n",
    "        #################################################################################   \n",
    "        #pcmci.print_results(results,self.alpha_level)\n",
    "        #pcmci.print_significant_links(p_matrix=results['p_matrix'],\n",
    "        #val_matrix = results['val_matrix'],\n",
    "        #alpha_level = self.alpha_level)\n",
    "        return results\n",
    "    \n",
    "    #################################################################################\n",
    "    # Step 2: Linear Regression\n",
    "    #################################################################################\n",
    "    # Helper functions\n",
    "    #################################################################################\n",
    "    def random_testindex(self,totalexp=None,testexp=None):\n",
    "        from numpy.random import default_rng\n",
    "        rng = default_rng(self.seed)\n",
    "        seed = rng.choice(totalexp, testexp, replace=False)\n",
    "        return seed\n",
    "    \n",
    "    def extract_lag_info(self,datar=None,varindex=None,lag=None):\n",
    "        temp = datar[:,varindex] # Full time series\n",
    "        store = []\n",
    "        for timeindex in range(len(temp)):\n",
    "            if timeindex < np.abs(lag):\n",
    "                store.append(np.nan)\n",
    "            elif timeindex > len(temp)-1-np.abs(lag):\n",
    "                store.append(np.nan)\n",
    "            else:\n",
    "                store.append(temp[timeindex-np.abs(lag)])\n",
    "        return store\n",
    "    \n",
    "    def extract_var_and_lag(self,pcmci_results=None,p_or_q='p' or 'q'):\n",
    "        datae = self.data.copy()\n",
    "        dataframe =  pp.DataFrame(datae,analysis_mode ='multiple', var_names=var_names)\n",
    "        pcmci = PCMCI(dataframe = dataframe, cond_ind_test = self.cond_ind_test)\n",
    "        #################################################################################\n",
    "        # Sel links\n",
    "        #################################################################################\n",
    "        for member in dataframe.values.keys():\n",
    "            children = [0,1,2]\n",
    "            parents = np.arange(3,259)\n",
    "            sel_links = select_links(self.tau_min0, self.tau_max0, parents, children)\n",
    "        \n",
    "        q_matrix = pcmci.get_corrected_pvalues(p_matrix=pcmci_results['p_matrix'],\n",
    "                                               selected_links = sel_links, \n",
    "                                               tau_min=self.tau_min0,tau_max=self.tau_max0, fdr_method='fdr_bh')\n",
    "        #################################################################################\n",
    "        # Save vars and lags\n",
    "        #################################################################################\n",
    "        if p_or_q=='p':\n",
    "            sig_links = (pcmci_results['p_matrix'].copy() <= self.alpha_level)\n",
    "        elif p_or_q=='q':\n",
    "            sig_links = (q_matrix.copy() <= self.alpha_level)\n",
    "        arr = []\n",
    "        for j in range(3):\n",
    "            links = {(p[0], -p[1]): np.abs(pcmci_results['val_matrix'][p[0],j,abs(p[1])]) for p in zip(*np.where(sig_links[:,j,:]))}\n",
    "            # Sort by value\n",
    "            sorted_links = sorted(links, key=links.get, reverse=True)\n",
    "            arr.append(sorted_links)\n",
    "        return arr\n",
    "    \n",
    "    #################################################################################\n",
    "    # Process functions\n",
    "    #################################################################################\n",
    "    def preproc_ts_withnan(self,links=None,group=None,trainmean=None,trainstd=None):\n",
    "        if self.target=='precip':\n",
    "            arrtarget,ytarget = links[0],[self.data[obj][:,0] for obj in self.data.keys()]\n",
    "        elif self.target=='pmin':\n",
    "            arrtarget,ytarget = links[1],[self.data[obj][:,1] for obj in self.data.keys()]\n",
    "        elif self.target=='v10':\n",
    "            arrtarget,ytarget = links[2],[self.data[obj][:,2] for obj in self.data.keys()]\n",
    "        \n",
    "        # 1. Extract time series with nan\n",
    "        varindexstore,lagstore,TS_store,flatTS_store = [],[],[],[]\n",
    "        for varindex,lag in arrtarget:\n",
    "            varindexstore.append(varindex)\n",
    "            lagstore.append(lag)\n",
    "            tempp = [self.extract_lag_info(datar=self.data[obj],varindex=varindex,lag=lag) for obj in self.data.keys()]\n",
    "            TS_store.append(tempp)\n",
    "            flatTS_store.append(np.concatenate([obj for obj in tempp]))\n",
    "        \n",
    "        # 2. Normalize\n",
    "        if group=='train':\n",
    "            TSnorml_store,meanstore,stdstore = [],[],[]\n",
    "            for i in range(len(TS_store)):\n",
    "                tempmean,tempstd = np.nanmean(flatTS_store[i]),np.nanstd(flatTS_store[i])\n",
    "                TSnorml_store.append([(obj-tempmean)/tempstd for obj in TS_store[i]])\n",
    "                meanstore.append(tempmean)\n",
    "                stdstore.append(tempstd)\n",
    "        elif (group=='test') or (group=='valid'):\n",
    "            TSnorml_store = []\n",
    "            for i in range(len(TS_store)):\n",
    "                tempmean,tempstd = (trainmean[i]),(trainstd[i])\n",
    "                TSnorml_store.append([(obj-tempmean)/tempstd for obj in TS_store[i]])\n",
    "            \n",
    "        # 3. Concatenate\n",
    "        #validindex,testindex = validtestindex[0:int(np.round(len(validtestindex)/2))],\\\n",
    "        #validtestindex[int(np.round(len(validtestindex)/2)):]\n",
    "        \n",
    "        Xtrain_withnan_store = []\n",
    "        for i in range(len(TS_store)):\n",
    "            storelist = [i for j, i in enumerate(TSnorml_store[i])]\n",
    "            Xtrain_withnan_store.append(np.concatenate([obj for obj in storelist]))\n",
    "            \n",
    "        Ytrain = np.concatenate([i for j, i in enumerate(ytarget)])\n",
    "        if group=='train':\n",
    "            return Xtrain_withnan_store,Ytrain,meanstore,stdstore\n",
    "        elif group=='valid':\n",
    "            return Xtrain_withnan_store,Ytrain\n",
    "        elif group=='test':\n",
    "            return Xtrain_withnan_store,Ytrain\n",
    "    \n",
    "    def remove_nan(self,Xdict=None,ydict=None):\n",
    "        def _remove_nan(X=None,y=None):\n",
    "            X_nonan,Y_nonan = [],[]\n",
    "            X_withnan_storer = np.asarray(X).transpose()\n",
    "            for i in (range(len(y))):\n",
    "                tempX = X_withnan_storer[i,:]\n",
    "                if np.isnan(tempX).any():\n",
    "                    continue\n",
    "                else:\n",
    "                    X_nonan.append(tempX)\n",
    "                    Y_nonan.append(y[i])\n",
    "            return X_nonan,Y_nonan\n",
    "        Xtrain,ytrain = _remove_nan(X=Xdict['train'],y=ydict['train'])\n",
    "        Xvalid,yvalid = _remove_nan(X=Xdict['valid'],y=ydict['valid'])\n",
    "        Xtest,ytest = _remove_nan(X=Xdict['test'],y=ydict['test'])\n",
    "        return {'train':Xtrain,'valid':Xvalid,'test':Xtest},{'train':ytrain,'valid':yvalid,'test':ytest}\n",
    "\n",
    "    #################################################################################\n",
    "    # Train model\n",
    "    #################################################################################\n",
    "    def train_mlr(self,X=None,y=None):\n",
    "        regr = LinearRegression()\n",
    "        regr.fit(X['train'],y['train'])\n",
    "        return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5aa803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLR_target(target='precip' or 'pmin' or 'v10',seed=12345):\n",
    "    ################################################################################################################\n",
    "    # 1. X,y with nan\n",
    "    Xtrain_nan,ytrain_nan,trainmean,trainstd = Pipeline(traindata,pc_alpha,alpha_level,pc_type='pcmci',\\\n",
    "                                                        tau_min0=1,tau_max0=16,\\\n",
    "                                                        target=target,var_name=var_names,\\\n",
    "                                                        seed=seed).preproc_ts_withnan(links=var_and_lag,group='train',\\\n",
    "                                                                                       trainmean=None,trainstd=None)\n",
    "    Xvalid_nan,yvalid_nan = Pipeline(validdata,pc_alpha,alpha_level,pc_type='pcmci',\\\n",
    "                                   tau_min0=1,tau_max0=16,target=target,\\\n",
    "                                   var_name=var_names,seed=seed).preproc_ts_withnan(links=var_and_lag,\\\n",
    "                                                                                     group='valid',\\\n",
    "                                                                                     trainmean=trainmean,\\\n",
    "                                                                                     trainstd=trainstd)\n",
    "    \n",
    "    Xtest_nan,ytest_nan = Pipeline(testdata,pc_alpha,alpha_level,pc_type='pcmci',\\\n",
    "                                   tau_min0=1,tau_max0=16,target=target,\\\n",
    "                                   var_name=var_names,seed=seed).preproc_ts_withnan(links=var_and_lag,\\\n",
    "                                                                                     group='test',\\\n",
    "                                                                                     trainmean=trainmean,\\\n",
    "                                                                                     trainstd=trainstd)\n",
    "    X_nan = {'train':Xtrain_nan,'valid':Xvalid_nan,'test':Xtest_nan}\n",
    "    y_nan = {'train':ytrain_nan,'valid':yvalid_nan,'test':ytest_nan}\n",
    "    ################################################################################################################\n",
    "    # 2. remove nan\n",
    "    X_nonan,y_nonan = Pipeline(traindata,pc_alpha,alpha_level,pc_type='pcmci',\\\n",
    "                               tau_min0=1,tau_max0=16,target=target,\\\n",
    "                               var_name=var_names,seed=seed).remove_nan(Xdict=X_nan,ydict=y_nan)\n",
    "    ################################################################################################################\n",
    "    # 3. Train MLR\n",
    "    wpac_mlr = Pipeline(traindata,pc_alpha,alpha_level,pc_type='pcmci',\\\n",
    "                        tau_min0=1,tau_max0=16,target=target,\\\n",
    "                        var_name=var_names,seed=seed).train_mlr(X=X_nonan,y=y_nonan)\n",
    "    ################################################################################################################\n",
    "    return wpac_mlr,X_nonan,y_nonan\n",
    "\n",
    "import pickle\n",
    "def save_to_pickle(loc=None,var=None):\n",
    "    with open(loc,\"wb\") as f:\n",
    "        pickle.dump(var,f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9e4a2",
   "metadata": {},
   "source": [
    "#### Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "076035a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddnio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e3d8e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970effee1b3547ac843a366d13851bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pc_alphaa,alpha_levellist,splitsize,seednum=0.01,[0.2,0.15,0.1,0.01,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],6,12348\n",
    "#[2e-1,1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9]\n",
    "var_names=dt1.columns.values.tolist()\n",
    "for pc_alpha in tqdm([0.2,0.1,0.05,0.01,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8]):\n",
    "    for alpha_level in (alpha_levellist):\n",
    "        testindex = (Pipeline(ddnio,pc_alpha,alpha_level,pc_type='pcmci',tau_min0=1,tau_max0=16,\\\n",
    "                              target=None,var_name=var_names,seed=seednum).random_testindex(24,splitsize))\n",
    "        traindata,validdata,testdata = Pipeline(ddnio,pc_alpha,alpha_level,pc_type='pcmci',tau_min0=1,tau_max0=16,\\\n",
    "                                                target='precip',var_name=var_names,seed=seednum).splitdata(testindex,'NIO')\n",
    "        result = Pipeline(traindata,pc_alpha,alpha_level,pc_type='pcmci',tau_min0=1,tau_max0=16,\\\n",
    "                          target='precip',var_name=var_names,seed=seednum).run_tigramite(int(24-splitsize))\n",
    "        var_and_lag = Pipeline(traindata,pc_alpha,alpha_level,pc_type='pcmci',tau_min0=1,tau_max0=16,\\\n",
    "                               target='precip',var_name=var_names,seed=seednum).extract_var_and_lag(result,p_or_q='p')\n",
    "        wpac_mlr_precip,X_precip,y_precip = trainMLR_target(target='precip',seed=seednum)\n",
    "        wpac_mlr_pmin,X_pmin,y_pmin = trainMLR_target(target='pmin',seed=seednum)\n",
    "        wpac_mlr_v10,X_v10,y_v10 = trainMLR_target(target='v10',seed=seednum)\n",
    "        save_to_pickle(loc='./pickleddata4/'+str(seednum)+'/causalnio_precip.obj.'+\\\n",
    "                       str(pc_alpha)+'.'+str(alpha_level)+'.'+str(seednum),\\\n",
    "                       var={'mlr':wpac_mlr_precip,'X':X_precip,'y':y_precip})\n",
    "        save_to_pickle(loc='./pickleddata4/'+str(seednum)+'/causalnio_pmin.obj.'+\\\n",
    "                       str(pc_alpha)+'.'+str(alpha_level)+'.'+str(seednum),\\\n",
    "                       var={'mlr':wpac_mlr_pmin,'X':X_pmin,'y':y_pmin})\n",
    "        save_to_pickle(loc='./pickleddata4/'+str(seednum)+'/causalnio_v10.obj.'+\\\n",
    "                       str(pc_alpha)+'.'+str(alpha_level)+'.'+str(seednum),\\\n",
    "                       var={'mlr':wpac_mlr_v10,'X':X_v10,'y':y_v10})\n",
    "        save_to_pickle(loc='./pickleddata4/'+str(seednum)+'/causalnio_lag_and_links.'+\\\n",
    "                       str(pc_alpha)+'.'+str(alpha_level)+'.'+str(seednum),var=var_and_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af41d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
