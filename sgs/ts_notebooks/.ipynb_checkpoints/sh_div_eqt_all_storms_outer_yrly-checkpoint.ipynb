{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4dd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8f7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/Data/ECMWF/ERA5_25kmx3hr/\"\n",
    "path=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/besttracks/\"\n",
    "output=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/create_ts/outputs/\"\n",
    "target=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/create_ts/outputs/targets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1f9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "track = sorted(glob.glob(path+'*2010*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a165ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracksDF = pd.read_csv(track[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8827a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracksDF['name'].unique()\n",
    "stormnames = list(tracksDF['name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ad1858",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010-03-19',\n",
       " '2010-03-19',\n",
       " '2010-03-19',\n",
       " '2010-03-19',\n",
       " '2010-03-19',\n",
       " '2010-03-19',\n",
       " '2010-03-20',\n",
       " '2010-03-20',\n",
       " '2010-03-20',\n",
       " '2010-03-20',\n",
       " '2010-03-20',\n",
       " '2010-03-20',\n",
       " '2010-03-20',\n",
       " '2010-03-20',\n",
       " '2010-03-21',\n",
       " '2010-03-21',\n",
       " '2010-03-21',\n",
       " '2010-03-21',\n",
       " '2010-03-21',\n",
       " '2010-03-21',\n",
       " '2010-03-21',\n",
       " '2010-03-21',\n",
       " '2010-03-22',\n",
       " '2010-03-22',\n",
       " '2010-03-22',\n",
       " '2010-03-22',\n",
       " '2010-03-22',\n",
       " '2010-03-22',\n",
       " '2010-03-22',\n",
       " '2010-03-22',\n",
       " '2010-03-23',\n",
       " '2010-03-23',\n",
       " '2010-03-23',\n",
       " '2010-03-23',\n",
       " '2010-03-23',\n",
       " '2010-03-23',\n",
       " '2010-03-23',\n",
       " '2010-03-23',\n",
       " '2010-03-24',\n",
       " '2010-03-24',\n",
       " '2010-03-24',\n",
       " '2010-03-24',\n",
       " '2010-03-24',\n",
       " '2010-03-24',\n",
       " '2010-03-24',\n",
       " '2010-03-24',\n",
       " '2010-03-25',\n",
       " '2010-03-25',\n",
       " '2010-03-25',\n",
       " '2010-03-25',\n",
       " '2010-03-25',\n",
       " '2010-03-25',\n",
       " '2010-03-25',\n",
       " '2010-03-25',\n",
       " '2010-03-26',\n",
       " '2010-03-26',\n",
       " '2010-03-26',\n",
       " '2010-03-26',\n",
       " '2010-03-26',\n",
       " '2010-03-26',\n",
       " '2010-03-26']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(tracksDF[tracksDF['name']==stormnames[0]].time[i]).split(':')[0] for i in range(len(tracksDF[tracksDF['name']==stormnames[0]].time))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9db457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000_200, 1000_300, 1000_500, 1000_700, 1000_850, 850_200, 850_250, 850_300, 850_500, 925_200, 925_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3215057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e988cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssingle level variables\n",
    "def output_indices(TCtrack=None,ERA5date=None,ERA5hour=None):\n",
    "    allindices = []\n",
    "    for timeidx in range(len(TCtrack)):#len(track['time'])):\n",
    "        datetrack,hourtrack = TCtrack['time'][timeidx].split(':')[0],TCtrack['time'][timeidx].split(':')[1][0:2]\n",
    "        ####################################################################################################\n",
    "        # Find the indices in ERA5 data with the same date as track\n",
    "        ####################################################################################################\n",
    "        dateind = []\n",
    "        for ind,obj in enumerate(ERA5date):\n",
    "            if obj==datetrack:\n",
    "                dateind.append(ind)\n",
    "        del ind,obj\n",
    "        hourind = []\n",
    "        hourextract = ERA5hour[int(np.min(np.asarray(dateind))):int(np.max(np.asarray(dateind)))+1]\n",
    "        for ind,obj in enumerate(hourextract):\n",
    "            if obj==hourtrack:            \n",
    "                hourind.append(ind)\n",
    "        allindices.append((int(np.min(np.asarray(dateind))),int(hourind[0])))\n",
    "    return allindices\n",
    "\n",
    "def extract_var(dataset=None,var='var138',indices=None):\n",
    "    extractedvar = []\n",
    "    for i in (range(len(indices))):\n",
    "        realindex = indices[i][0]+indices[i][1]\n",
    "        extractedvar.append(dataset[var][int(realindex),...].data)\n",
    "    return np.asarray(extractedvar)\n",
    "\n",
    "def largearea(dataset=None,invar=None,indices=None):\n",
    "    if len(invar.shape) != 3:\n",
    "        invar = np.squeeze(invar)\n",
    "    ds = xr.Dataset(\n",
    "    data_vars=dict(variable=([\"time\",\"lat\",\"lon\"], invar)),#mysvar[0])),\n",
    "    coords=dict(lat=([\"lat\"], dataset.lat.data),lon=([\"lon\"], dataset.lon.data),time=([\"time\"], np.linspace(0,len(indices)-1,len(indices)))),\n",
    "    attrs=dict(description=\"coords with matrices\"),)\n",
    "    \n",
    "    var_out=np.zeros((len(indices),64,64))\n",
    "    for it in range(len(indices)):\n",
    "        latn, lats, lone, lonw = tc_orad[it,:]\n",
    "        try:\n",
    "            var_out[it,:,:]=ds['variable'][it,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))\n",
    "        except:\n",
    "            var_out[it,:,:]=ds['variable'][it,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))[0:64,0:64]\n",
    "    return var_out\n",
    "\n",
    "def largearea_withpres(dataset=None,invar=None,indices=None):\n",
    "    ds = xr.Dataset(\n",
    "    data_vars=dict(variable=([\"time\",\"plev\",\"lat\",\"lon\"], invar)),#mysvar[0])),\n",
    "    coords=dict(lat=([\"lat\"], dataset.lat.data),lon=([\"lon\"], dataset.lon.data),time=([\"time\"], np.linspace(0,len(indices)-1,len(indices))),\n",
    "               plev=([\"plev\"],dataset.plev.data)),\n",
    "    attrs=dict(description=\"coords with matrices\"),)\n",
    "    var_out=np.zeros((len(indices),len(dm2.plev.data),64,64))\n",
    "    for it in range(len(indices)):\n",
    "        latn, lats, lone, lonw = tc_orad[it,:]\n",
    "        for ip in range(len(dm2.plev.data)):\n",
    "            try:\n",
    "                var_out[it,ip,:,:]=ds['variable'][it,ip,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))\n",
    "            except:\n",
    "                var_out[it,ip,:,:]=ds['variable'][it,ip,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))[0:64,0:64]\n",
    "    return var_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "731e8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Shear vars\n",
    "def createmask(dm=None,irad=None,orad=None,lonselect=None,latselect=None):\n",
    "    mask = []\n",
    "    for ti in range(len(tc_irad)):\n",
    "        lonselect = dm.lon.sel(lon=slice(orad[ti,:][2],orad[ti,:][3])).data\n",
    "        latselect = np.flipud(dm.lat.sel(lat=slice(orad[ti,:][1],orad[ti,:][0])).data)\n",
    "        if (lonselect.shape != 64) or (latselect.shape != 64):\n",
    "            lon2d,lat2d = np.meshgrid(lonselect[0:64],latselect[0:64])\n",
    "        else:\n",
    "            lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "        #############################################################################################\n",
    "        latcriteria = np.logical_and(lat2d>irad[ti][0],lat2d<irad[ti][1])\n",
    "        loncriteria = np.logical_and(lon2d>irad[ti][2],lon2d<irad[ti][3])\n",
    "        allcriteria = np.logical_and(loncriteria,latcriteria)\n",
    "        mask.append(allcriteria)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readyear_automatic(year=None):\n",
    "    dm2 = xr.open_dataset(datapath+'/slev_vars/svars_'+str(year)+'.nc')\n",
    "    #tracklist = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/besttracks/nio/*_'+str(year)+'*'))\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,era5_date,era5_hour\n",
    "\n",
    "dm2,era5_date,era5_hour = readyear_automatic(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678af7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readyear(year=None,sheartype=None):\n",
    "    dm2 = xr.open_dataset(datapath+'/shear/shear_'+str(sheartype)+'_'+str(year)+'.nc')\n",
    "    #tracklist = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/besttracks/wp/*_'+str(year)+'*'))\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,era5_date,era5_hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260a013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb11457d2bce435a93b4574253a88051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e76058dda04caf90a2494c9514202f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403e31bf7384417aaf4c37cd2e41db80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f22ccfe0344e718e7a1cf2def8175f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01213d1d726248f999f7352b730b52c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cab93bf1b094cf0ba55a8963ba5334f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd911e5d4a4401fa53da0792a3595c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b09d13698f84c1d91729fa1be923d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca30794b5f146a7bf5c11cdf65bd5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7449a7142946f68a5d9451aaf71931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704dd12192e44519950c1a91082b9050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCshears = {}\n",
    "for shear in ['1000_200', '1000_300', '1000_500', '1000_700', '1000_850', \\\n",
    "              '850_200', '850_250', '850_300', '850_500', '925_200', '925_250']:\n",
    "    dm2,era5_date,era5_hour = readyear(2010,shear)\n",
    "    TCtempstore = []\n",
    "    for TCobj in tqdm(stormnames):\n",
    "        track=tracksDF[tracksDF['name']==TCobj].reset_index()\n",
    "        lon1=track['lon'].to_numpy()\n",
    "        lat1=track['lat'].to_numpy()\n",
    "        lonx=np.mod(lon1,360)\n",
    "        pos = arr = np.stack((lat1, lonx), axis=1)\n",
    "        ###########################################################################\n",
    "        indices_store = output_indices(track,era5_date,era5_hour)\n",
    "        ###########################################################################\n",
    "        mysvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "        ###########################################################################\n",
    "        tc_irad=np.empty((len(indices_store),4))\n",
    "        tc_irad[:,0] = pos[:,0]-2\n",
    "        tc_irad[:,1] = pos[:,0]+2\n",
    "        tc_irad[:,2] = pos[:,1]-2\n",
    "        tc_irad[:,3] = pos[:,1]+2\n",
    "        \n",
    "        tc_orad=np.empty((len(indices_store),4))\n",
    "        tc_orad[:,0] = pos[:,0]-8\n",
    "        tc_orad[:,1] = pos[:,0]+8\n",
    "        tc_orad[:,2] = pos[:,1]-8\n",
    "        tc_orad[:,3] = pos[:,1]+8\n",
    "        ###########################################################################\n",
    "        smallsvarout = [largearea(dm2,mysvar[i],indices_store) for i in (range(len(mysvar)))]\n",
    "        \n",
    "        svarname1 = ['shear_'+shear]#['shear_850_200']\n",
    "        svarname = ['shear']\n",
    "        \n",
    "        svardict = {varnameobj:varobj for (varnameobj,varobj) in zip(svarname,smallsvarout)}\n",
    "        lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "        latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "        lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "        \n",
    "        mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "        #############################################################################################\n",
    "        tsdict = {}\n",
    "        for ind,obj in (enumerate(svarname)):\n",
    "            tslist = [svardict[svarname[ind]][i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "            tsdict[svarname1[ind]] = [np.nanmean(obj) for obj in tslist]\n",
    "        #############################################################################################\n",
    "        TCtempstore.append(tsdict)\n",
    "    TCshears[shear] = TCtempstore\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b7736d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallTCshear1 = {}\n",
    "smallTCshear2 = {}\n",
    "smallTCshear3 = {}\n",
    "smallTCshear4 = {}\n",
    "smallTCshear5 = {}\n",
    "smallTCshear6 = {}\n",
    "smallTCshear7 = {}\n",
    "smallTCshear8 = {}\n",
    "smallTCshear9 = {}\n",
    "smallTCshear10 = {}\n",
    "smallTCshear11 = {}\n",
    "smallTCshear12 = {}\n",
    "smallTCshear13 = {}\n",
    "\n",
    "\n",
    "for shear in ['1000_200', '1000_300', '1000_500', '1000_700', '1000_850', \\\n",
    "              '850_200', '850_250', '850_300', '850_500', '925_200', '925_250']:\n",
    "    smallTCshear1['shear_'+str(shear)] = (TCshears[shear][0]['shear_'+str(shear)])\n",
    "    smallTCshear2['shear_'+str(shear)] = (TCshears[shear][1]['shear_'+str(shear)])\n",
    "    smallTCshear3['shear_'+str(shear)] = (TCshears[shear][2]['shear_'+str(shear)])\n",
    "    smallTCshear4['shear_'+str(shear)] = (TCshears[shear][3]['shear_'+str(shear)])\n",
    "    smallTCshear5['shear_'+str(shear)] = (TCshears[shear][4]['shear_'+str(shear)])\n",
    "    smallTCshear6['shear_'+str(shear)] = (TCshears[shear][5]['shear_'+str(shear)])\n",
    "    smallTCshear7['shear_'+str(shear)] = (TCshears[shear][6]['shear_'+str(shear)])\n",
    "    smallTCshear8['shear_'+str(shear)] = (TCshears[shear][7]['shear_'+str(shear)])\n",
    "    smallTCshear9['shear_'+str(shear)] = (TCshears[shear][8]['shear_'+str(shear)])\n",
    "    smallTCshear10['shear_'+str(shear)] = (TCshears[shear][9]['shear_'+str(shear)])\n",
    "    smallTCshear11['shear_'+str(shear)] = (TCshears[shear][10]['shear_'+str(shear)])\n",
    "    smallTCshear12['shear_'+str(shear)] = (TCshears[shear][11]['shear_'+str(shear)])\n",
    "    smallTCshear13['shear_'+str(shear)] = (TCshears[shear][12]['shear_'+str(shear)])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce9dba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "shears1=pd.DataFrame.from_dict(smallTCshear1)\n",
    "shears2=pd.DataFrame.from_dict(smallTCshear2)\n",
    "shears3=pd.DataFrame.from_dict(smallTCshear3)\n",
    "shears4=pd.DataFrame.from_dict(smallTCshear4)\n",
    "shears5=pd.DataFrame.from_dict(smallTCshear5)\n",
    "shears6=pd.DataFrame.from_dict(smallTCshear6)\n",
    "shears7=pd.DataFrame.from_dict(smallTCshear7)\n",
    "shears8=pd.DataFrame.from_dict(smallTCshear8)\n",
    "shears9=pd.DataFrame.from_dict(smallTCshear9)\n",
    "shears10=pd.DataFrame.from_dict(smallTCshear10)\n",
    "shears11=pd.DataFrame.from_dict(smallTCshear11)\n",
    "shears12=pd.DataFrame.from_dict(smallTCshear12)\n",
    "shears13=pd.DataFrame.from_dict(smallTCshear13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab647582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OMAIS',\n",
       " 'CONSON',\n",
       " 'CHANTHU',\n",
       " 'DIANMU',\n",
       " 'LIONROCK',\n",
       " 'KOMPASU',\n",
       " 'MALOU',\n",
       " 'MERANTI',\n",
       " 'FANAPI',\n",
       " 'MALAKAS',\n",
       " 'MEGI',\n",
       " 'CHABA',\n",
       " 'OMEKA']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stormnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980ea7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee56ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "shears1.to_csv(output+'shears_wpouter_2010_OMAIS.csv')\n",
    "shears2.to_csv(output+'shears_wpouter_2010_CONSON.csv')\n",
    "shears3.to_csv(output+'shears_wpouter_2010_CHANTHU.csv')\n",
    "shears4.to_csv(output+'shears_wpouter_2010_DIANMU.csv')\n",
    "shears5.to_csv(output+'shears_wpouter_2010_LIONROCK.csv')\n",
    "shears6.to_csv(output+'shears_wpouter_2010_KOMPASU.csv')\n",
    "shears7.to_csv(output+'shears_wpouter_2010_MALOU.csv')\n",
    "shears8.to_csv(output+'shears_wpouter_2010_MERANTI.csv')\n",
    "shears9.to_csv(output+'shears_wpouter_2010_FANAPI.csv')\n",
    "shears10.to_csv(output+'shears_wpouter_2010_MALAKAS.csv')\n",
    "shears11.to_csv(output+'shears_wpouter_2010_MEGI.csv')\n",
    "shears12.to_csv(output+'shears_wpouter_2010_CHABA.csv')\n",
    "shears13.to_csv(output+'shears_wpouter_2010_OMEKA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35d503e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallTCshear1 = {}\n",
    "for shear in ['1000_200', '1000_300', '1000_500', '1000_700', '1000_850', \\\n",
    "              '850_200', '850_250', '850_300', '850_500', '925_200', '925_250']:\n",
    "    smallTCshear1['shear_'+str(shear)] = (TCshears[shear][0]['shear_'+str(shear)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e647449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e46f9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smallTCshear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528f3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "286286b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smallTCshear1 = {}\n",
    "#smallTCshear2 = {}\n",
    "#smallTCshear3 = {}\n",
    "#smallTCshear4 = {}\n",
    "#smallTCshear5 = {}\n",
    "#smallTCshear6 = {}\n",
    "#smallTCshear7 = {}\n",
    "#smallTCshear8 = {}\n",
    "\n",
    "for shear in ['1000_200', '1000_300', '1000_500', '1000_700', '1000_850', \\\n",
    "              '850_200', '850_250', '850_300', '850_500', '925_200', '925_250']:\n",
    "    smallTCshear1['shear_'+str(shear)] = (TCshears[shear][0]['shear_'+str(shear)])\n",
    "   # smallTCshear2['shear_'+str(shear)] = (TCshears[shear][1]['shear_'+str(shear)])\n",
    "   # smallTCshear3['shear_'+str(shear)] = (TCshears[shear][2]['shear_'+str(shear)])\n",
    "   # smallTCshear4['shear_'+str(shear)] = (TCshears[shear][3]['shear_'+str(shear)])\n",
    "   # smallTCshear5['shear_'+str(shear)] = (TCshears[shear][4]['shear_'+str(shear)])\n",
    "   # smallTCshear6['shear_'+str(shear)] = (TCshears[shear][5]['shear_'+str(shear)])\n",
    "   # smallTCshear7['shear_'+str(shear)] = (TCshears[shear][6]['shear_'+str(shear)])\n",
    "   # smallTCshear8['shear_'+str(shear)] = (TCshears[shear][7]['shear_'+str(shear)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "aa81e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shears1=pd.DataFrame.from_dict(smallTCshear1)\n",
    "#shears2=pd.DataFrame.from_dict(smallTCshear2)\n",
    "#shears3=pd.DataFrame.from_dict(smallTCshear3)\n",
    "#shears4=pd.DataFrame.from_dict(smallTCshear4)\n",
    "#shears5=pd.DataFrame.from_dict(smallTCshear5)\n",
    "#shears6=pd.DataFrame.from_dict(smallTCshear6)\n",
    "#shears7=pd.DataFrame.from_dict(smallTCshear7)\n",
    "#shears8=pd.DataFrame.from_dict(smallTCshear8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d9daf5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/work/FAC/FGSE/IDYST/tbeucler/default/saranya/causal/besttracks/wp/nwp_20101011.csv']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "18328650",
   "metadata": {},
   "outputs": [],
   "source": [
    "shears1.to_csv(output+'shears_wpouter_20101011.csv')\n",
    "#shears2.to_csv(output+'shears_wpouter_20110616.csv')\n",
    "#shears3.to_csv(output+'shears_wpouter_20110926.csv')\n",
    "#shears4.to_csv(output+'shears_wpouter_20111211.csv')\n",
    "#shears5.to_csv(output+'shears_wpouter_20111125.csv')\n",
    "#shears6.to_csv(output+'shears_wpouter_20131102.csv')\n",
    "#shears7.to_csv(output+'shears_wpouter_20191117.csv')\n",
    "#shears8.to_csv(output+'shears_wpouter_20191124.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f27c2ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2 = xr.open_dataset(datapath+'/div/div_2010.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d4d79db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88006a7d248145c8ae8522addc4b4150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pressure vars - divergence3.nc')\n",
    "TCdiv_ts = []\n",
    "for TCobj in tqdm(stormnames):\n",
    "    track=tracksDF[tracksDF['name']==TCobj].reset_index()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lonx=np.mod(lon1,360)\n",
    "    pos = arr = np.stack((lat1, lonx), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mypvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallpvarout = [largearea_withpres(dm2,mypvar[i],indices_store) for i in (range(len(mypvar)))]    \n",
    "    pvarname1 = ['outdiv']\n",
    "    pvarname = ['var155']\n",
    "    pvardict = {varnameobj:varobj for (varnameobj,varobj) in zip(pvarname,smallpvarout)}\n",
    "    \n",
    "    lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    #latselect = dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "    mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    ts_pdict = {}\n",
    "    for ind,obj in (enumerate(pvarname)):\n",
    "        pvarTS_store = []\n",
    "        for plevv in range(len(dm2.plev.data)):\n",
    "            tempvar = pvardict[pvarname[ind]][:,plevv,...]\n",
    "            tempts = [tempvar[i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "            tempTSERIES = [np.nanmean(obj) for obj in tempts]\n",
    "            pvarTS_store.append(tempTSERIES)\n",
    "        ts_pdict[pvarname1[ind]] = np.asarray(pvarTS_store).transpose()\n",
    "    #############################################################################################\n",
    "    TCdiv_ts.append(ts_pdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71513145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plv = [str(int(obj)) for obj in dm2.plev.data/100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5614dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pandadict(divdict=None,divlv=plv):\n",
    "    tempdivdict = {'outdiv_'+divlv[i]:divdict['outdiv'][:,i] for i in range(divdict['outdiv'].shape[1])}\n",
    "    alldict = {**tempdivdict}\n",
    "    return alldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "924794a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "storeTCdicts = {}\n",
    "for ind in range(len(TCdiv_ts)):\n",
    "    storeTCdicts[stormnames[ind]] = pd.DataFrame.from_dict(create_pandadict(divdict=TCdiv_ts[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe9c3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,obj in enumerate(stormnames):\n",
    "    storeTCdicts[obj].to_csv(output+'2010_outerdiv_wpac_'+str(obj)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362633f0",
   "metadata": {},
   "source": [
    "Eq. potential temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c34d0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readyear(year=None):\n",
    "    dm2 = xr.open_mfdataset([\n",
    "                             datapath+'/eqt/eqt_200_'+str(year)+'.nc',datapath+'/eqt/eqt_250_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_300_'+str(year)+'.nc',datapath+'/eqt/eqt_400_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_500_'+str(year)+'.nc',datapath+'/eqt/eqt_600_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_700_'+str(year)+'.nc',datapath+'/eqt/eqt_800_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_850_'+str(year)+'.nc',datapath+'/eqt/eqt_1000_'+str(year)+'.nc',\\\n",
    "                             datapath+'/eqt/eqt_925_'+str(year)+'.nc'])\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,era5_date,era5_hour\n",
    "\n",
    "dm2,era5_date,era5_hour = readyear(2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4435108d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60387eebc255490195bbbd5ab961f995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TCeqt_ts = []\n",
    "for TCobj in tqdm(stormnames):\n",
    "    track=tracksDF[tracksDF['name']==TCobj].reset_index()\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mysvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallsvarout = [largearea(dm2,mysvar[i],indices_store) for i in (range(len(mysvar)))]\n",
    "    svarname1 = ['outeqt1000','outeqt200','outeqt250','outeqt300','outeqt400',\\\n",
    "                 'outeqt500','outeqt600','outeqt700','outeqt800',\\\n",
    "                 'outeqt850','outeqt925']\n",
    "   \n",
    "    svarname = ['eqt1000','eqt200','eqt250','eqt300','eqt400',\\\n",
    "                'eqt500','eqt600','eqt700','eqt800',\\\n",
    "                'eqt850','eqt925']\n",
    "    \n",
    "    \n",
    "    svardict = {varnameobj:varobj for (varnameobj,varobj) in zip(svarname,smallsvarout)}\n",
    "    \n",
    "    lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "\n",
    "    mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    tsdict = {}\n",
    "    for ind,obj in (enumerate(svarname)):\n",
    "        tslist = [svardict[svarname[ind]][i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "        tsdict[svarname1[ind]] = [np.nanmean(obj) for obj in tslist]\n",
    "    #############################################################################################\n",
    "    TCeqt_ts.append(tsdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97bc0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "storeTCdicts = {}\n",
    "for ind in range(len(TCeqt_ts)):\n",
    "    storeTCdicts[stormnames[ind]] = pd.DataFrame.from_dict(TCeqt_ts[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47d5af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,obj in enumerate(stormnames):\n",
    "    storeTCdicts[obj].to_csv(output+'2010_outereqt_wpac_'+str(obj)+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
