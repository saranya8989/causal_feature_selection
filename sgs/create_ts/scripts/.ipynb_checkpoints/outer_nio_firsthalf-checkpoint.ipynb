{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdbd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76480d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/Data/ECMWF/ERA5_25kmx3hr/\"\n",
    "path=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/\"\n",
    "output=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/outputs/\"\n",
    "target=\"/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/outputs/targets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc254b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracklist = sorted(glob.glob(path+'tracks_nio/*2010*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418d4132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/tracks_nio/nio_20100515.csv',\n",
       " '/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/tracks_nio/nio_20100530.csv',\n",
       " '/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/tracks_nio/nio_20101031.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed355dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm1=xr.open_mfdataset([datapath+'/vor/vor_2010.nc',datapath+'/rhum/rhum_2010.nc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b848782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssingle level variables\n",
    "def output_indices(TCtrack=None,ERA5date=None,ERA5hour=None):\n",
    "    allindices = []\n",
    "    for timeidx in range(len(TCtrack)):#len(track['time'])):\n",
    "        datetrack,hourtrack = TCtrack['time'][timeidx].split(':')[0],TCtrack['time'][timeidx].split(':')[1][0:2]\n",
    "        ####################################################################################################\n",
    "        # Find the indices in ERA5 data with the same date as track\n",
    "        ####################################################################################################\n",
    "        dateind = []\n",
    "        for ind,obj in enumerate(ERA5date):\n",
    "            if obj==datetrack:\n",
    "                dateind.append(ind)\n",
    "        del ind,obj\n",
    "        hourind = []\n",
    "        hourextract = ERA5hour[int(np.min(np.asarray(dateind))):int(np.max(np.asarray(dateind)))+1]\n",
    "        for ind,obj in enumerate(hourextract):\n",
    "            if obj==hourtrack:            \n",
    "                hourind.append(ind)\n",
    "        allindices.append((int(np.min(np.asarray(dateind))),int(hourind[0])))\n",
    "    return allindices\n",
    "\n",
    "def extract_var(dataset=None,var='var138',indices=None):\n",
    "    extractedvar = []\n",
    "    for i in (range(len(indices))):\n",
    "        realindex = indices[i][0]+indices[i][1]\n",
    "        extractedvar.append(dataset[var][int(realindex),...].data)\n",
    "    return np.asarray(extractedvar)\n",
    "\n",
    "def largearea(dataset=None,invar=None,indices=None):\n",
    "    if len(invar.shape) != 3:\n",
    "        invar = np.squeeze(invar)\n",
    "    ds = xr.Dataset(\n",
    "    data_vars=dict(variable=([\"time\",\"lat\",\"lon\"], invar)),#mysvar[0])),\n",
    "    coords=dict(lat=([\"lat\"], dataset.lat.data),lon=([\"lon\"], dataset.lon.data),time=([\"time\"], np.linspace(0,len(indices)-1,len(indices)))),\n",
    "    attrs=dict(description=\"coords with matrices\"),)\n",
    "    \n",
    "    var_out=np.zeros((len(indices),64,64))\n",
    "    for it in range(len(indices)):\n",
    "        latn, lats, lone, lonw = tc_orad[it,:]\n",
    "        try:\n",
    "            var_out[it,:,:]=ds['variable'][it,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))\n",
    "        except:\n",
    "            var_out[it,:,:]=ds['variable'][it,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))[0:64,0:64]\n",
    "    return var_out\n",
    "\n",
    "def largearea_withpres(dataset=None,invar=None,indices=None):\n",
    "    ds = xr.Dataset(\n",
    "    data_vars=dict(variable=([\"time\",\"plev\",\"lat\",\"lon\"], invar)),#mysvar[0])),\n",
    "    coords=dict(lat=([\"lat\"], dataset.lat.data),lon=([\"lon\"], dataset.lon.data),time=([\"time\"], np.linspace(0,len(indices)-1,len(indices))),\n",
    "               plev=([\"plev\"],dataset.plev.data)),\n",
    "    attrs=dict(description=\"coords with matrices\"),)\n",
    "    var_out=np.zeros((len(indices),len(dm1.plev.data),64,64))\n",
    "    for it in range(len(indices)):\n",
    "        latn, lats, lone, lonw = tc_orad[it,:]\n",
    "        for ip in range(len(dm1.plev.data)):\n",
    "            try:\n",
    "                var_out[it,ip,:,:]=ds['variable'][it,ip,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))\n",
    "            except:\n",
    "                var_out[it,ip,:,:]=ds['variable'][it,ip,:,:].sel(lat=slice(lats,latn),lon=slice(lone,lonw))[0:64,0:64]\n",
    "    return var_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8002450c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac61a77ddc1f4f92840755569e84907f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###\n",
    "def createmask(dm=None,irad=None,orad=None,lonselect=None,latselect=None):\n",
    "    mask = []\n",
    "    for ti in range(len(tc_irad)):\n",
    "        lonselect = dm.lon.sel(lon=slice(orad[ti,:][2],orad[ti,:][3])).data\n",
    "        latselect = np.flipud(dm.lat.sel(lat=slice(orad[ti,:][1],orad[ti,:][0])).data)\n",
    "        if (lonselect.shape != 64) or (latselect.shape != 64):\n",
    "            lon2d,lat2d = np.meshgrid(lonselect[0:64],latselect[0:64])\n",
    "        else:\n",
    "            lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "        #############################################################################################\n",
    "        latcriteria = np.logical_and(lat2d>irad[ti][0],lat2d<irad[ti][1])\n",
    "        loncriteria = np.logical_and(lon2d>irad[ti][2],lon2d<irad[ti][3])\n",
    "        allcriteria = np.logical_and(loncriteria,latcriteria)\n",
    "        mask.append(allcriteria)\n",
    "    return mask\n",
    "\n",
    "def readyear(year=None):\n",
    "    dm2 = xr.open_dataset(datapath+'/slev_vars/svars_'+str(year)+'.nc')\n",
    "    tracklist = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/tracks_nio/*'+str(year)+'*'))\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,tracklist,era5_date,era5_hour\n",
    "\n",
    "dm2,tracklist,era5_date,era5_hour = readyear(2010)\n",
    "\n",
    "TCs_ts = []\n",
    "for TCobj in tqdm(tracklist):\n",
    "    track=pd.read_csv(TCobj,delimiter=r\",\")\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mysvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallsvarout = [largearea(dm2,mysvar[i],indices_store) for i in (range(len(mysvar)))]\n",
    "    svarname1 = ['outu10','outv10','out2mdewtmp','out2mtmp','outconv_ppt','outtot_cld_ice',\\\n",
    "           'outtot_cldwtr','outtot_cld_rain','outvi_div_cld_froz_wtr','outvi_div_cld_liq_wtr','outvi_div_gpot_flux',\\\n",
    "           'outvi_div_ke_flux','outvi_div_mass_flux','outvi_div_moisture_flux','outvi_div_olr_flux','outvi_div_tot_enrgy_flux',\\\n",
    "           'outvi_ke','outvi_pe_inte','outvi_pe_ie_latentenrgy','outvi_temp','outvi_olr','outvi_tot_enrgy','outvi_moisture_div']\n",
    "   \n",
    "    svarname = ['u10','v10','2mdewtmp','2mtmp','conv_ppt','tot_cld_ice',\\\n",
    "           'tot_cldwtr','tot_cld_rain','vi_div_cld_froz_wtr','vi_div_cld_liq_wtr','vi_div_gpot_flux',\\\n",
    "           'vi_div_ke_flux','vi_div_mass_flux','vi_div_moisture_flux','vi_div_olr_flux','vi_div_tot_enrgy_flux',\\\n",
    "           'vi_ke','vi_pe_inte','vi_pe_ie_latentenrgy','vi_temp','vi_olr','vi_tot_enrgy','vi_moisture_div']\n",
    "    svardict = {varnameobj:varobj for (varnameobj,varobj) in zip(svarname,smallsvarout)}\n",
    "    \n",
    "    lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "\n",
    "    mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    tsdict = {}\n",
    "    for ind,obj in (enumerate(svarname)):\n",
    "        tslist = [svardict[svarname[ind]][i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "        tsdict[svarname1[ind]] = [np.nanmean(obj) for obj in tslist]\n",
    "    #############################################################################################\n",
    "    TCs_ts.append(tsdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e901de2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafe157b21d64d2091c642d6679b2443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###radiation variables\n",
    "def createmask(dm=None,irad=None,orad=None,lonselect=None,latselect=None):\n",
    "    mask = []\n",
    "    for ti in range(len(tc_irad)):\n",
    "        lonselect = dm.lon.sel(lon=slice(orad[ti,:][2],orad[ti,:][3])).data\n",
    "        latselect = np.flipud(dm.lat.sel(lat=slice(orad[ti,:][1],orad[ti,:][0])).data)\n",
    "        if (lonselect.shape != 64) or (latselect.shape != 64):\n",
    "            lon2d,lat2d = np.meshgrid(lonselect[0:64],latselect[0:64])\n",
    "        else:\n",
    "            lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "        #############################################################################################\n",
    "        latcriteria = np.logical_and(lat2d>irad[ti][0],lat2d<irad[ti][1])\n",
    "        loncriteria = np.logical_and(lon2d>irad[ti][2],lon2d<irad[ti][3])\n",
    "        allcriteria = np.logical_and(loncriteria,latcriteria)\n",
    "        mask.append(allcriteria)\n",
    "    return mask\n",
    "\n",
    "def readyear(year=None):\n",
    "    dm2 = xr.open_dataset(datapath+'/slev_vars/radvars_'+str(year)+'.nc')\n",
    "    tracklist = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/tracks_nio/*'+str(year)+'*'))\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,tracklist,era5_date,era5_hour\n",
    "\n",
    "dm2,tracklist,era5_date,era5_hour = readyear(2010)\n",
    "\n",
    "TCr_ts = []\n",
    "for TCobj in tqdm(tracklist):\n",
    "    track=pd.read_csv(TCobj,delimiter=r\",\")\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mysvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallsvarout = [largearea(dm2,mysvar[i],indices_store) for i in (range(len(mysvar)))]\n",
    "    \n",
    "    svarname1 = ['outcape','outinst_10m_wnd_gst','outinst_moisture_flux','outinst_ssh_flux','outsurfmean_swr_flux','outsurfmean_lhf',\\\n",
    "           'outsurfmean_lwr_flux','outsurfmean_shf','outdwnwrdmean_swr_flux','outtopmean_lwr_flux','outtopmean_swr_flux',\\\n",
    "           'outvimean_moisture_div','outsurf_lhf','outsurf_shf','outtot_suprcool_liqwtr','outtot_wtr_vpr']\n",
    "    \n",
    "    \n",
    "    svarname = ['cape','inst_10m_wnd_gst','inst_moisture_flux','inst_ssh_flux','surfmean_swr_flux','surfmean_lhf',\\\n",
    "           'surfmean_lwr_flux','surfmean_shf','dwnwrdmean_swr_flux','topmean_lwr_flux','topmean_swr_flux',\\\n",
    "           'vimean_moisture_div','surf_lhf','surf_shf','tot_suprcool_liqwtr','tot_wtr_vpr']\n",
    "    \n",
    "    svardict = {varnameobj:varobj for (varnameobj,varobj) in zip(svarname,smallsvarout)}\n",
    "    \n",
    "    lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "\n",
    "    mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    tsdict = {}\n",
    "    for ind,obj in (enumerate(svarname)):\n",
    "        tslist = [svardict[svarname[ind]][i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "        tsdict[svarname1[ind]] = [np.nanmean(obj) for obj in tslist]\n",
    "    #############################################################################################\n",
    "    TCr_ts.append(tsdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########precip, mslp, sst variables############\n",
    "def createmask(dm=None,irad=None,orad=None,lonselect=None,latselect=None):\n",
    "    mask = []\n",
    "    for ti in range(len(tc_irad)):\n",
    "        lonselect = dm.lon.sel(lon=slice(orad[ti,:][2],orad[ti,:][3])).data\n",
    "        latselect = np.flipud(dm.lat.sel(lat=slice(orad[ti,:][1],orad[ti,:][0])).data)\n",
    "        if (lonselect.shape != 64) or (latselect.shape != 64):\n",
    "            lon2d,lat2d = np.meshgrid(lonselect[0:64],latselect[0:64])\n",
    "        else:\n",
    "            lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "        #############################################################################################\n",
    "        latcriteria = np.logical_and(lat2d>irad[ti][0],lat2d<irad[ti][1])\n",
    "        loncriteria = np.logical_and(lon2d>irad[ti][2],lon2d<irad[ti][3])\n",
    "        allcriteria = np.logical_and(loncriteria,latcriteria)\n",
    "        mask.append(allcriteria)\n",
    "    return mask\n",
    "\n",
    "def readyear(year=None):\n",
    "    dm2 = xr.open_mfdataset([datapath+'/prate/prates_2010.nc',datapath+'/mslp/mslp_2010.nc',datapath+'/sst/sst_2010.nc'])\n",
    "    tracklist = sorted(glob.glob('/work/FAC/FGSE/IDYST/tbeucler/default/saranya/create_ts/tracks_nio/*'+str(year)+'*'))\n",
    "    era5_date = [str(dm2.time[i].data).split('T')[0] for i in range(len(dm2.time))]\n",
    "    era5_hour = [str(dm2.time[i].data).split('T')[1][0:2] for i in range(len(dm2.time))]\n",
    "    return dm2,tracklist,era5_date,era5_hour\n",
    "\n",
    "dm2,tracklist,era5_date,era5_hour = readyear(2010)\n",
    "\n",
    "TCpr_ts = []\n",
    "for TCobj in tqdm(tracklist):\n",
    "    track=pd.read_csv(TCobj,delimiter=r\",\")\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mysvar = [extract_var(dataset=dm2,var=obj,indices=indices_store) for obj in (list(dm2.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallsvarout = [largearea(dm2,mysvar[i],indices_store) for i in (range(len(mysvar)))]\n",
    "    \n",
    "    svarname1 = ['outmslp','outconv_rrate','outls_rrate','outmn_conv_prate',\\\n",
    "                 'outmn_ls_prate','outmn_tot_prate','outsst']\n",
    "    \n",
    "    svarname = ['mslp','conv_rrate','ls_rrate','mn_conv_prate','mn_ls_prate','mn_tot_prate','sst']\n",
    "    \n",
    "    svardict = {varnameobj:varobj for (varnameobj,varobj) in zip(svarname,smallsvarout)}\n",
    "    \n",
    "    lonselect = dm2.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm2.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "\n",
    "    mymask = createmask(dm=dm2,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    tsdict = {}\n",
    "    for ind,obj in (enumerate(svarname)):\n",
    "        tslist = [svardict[svarname[ind]][i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "        tsdict[svarname1[ind]] = [np.nanmean(obj) for obj in tslist]\n",
    "    #############################################################################################\n",
    "    TCpr_ts.append(tsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5d4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressure vars - vor hum\n",
    "TCp_ts = []\n",
    "for TCobj in tqdm(tracklist):\n",
    "    track=pd.read_csv(TCobj,delimiter=r\",\")\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mypvar = [extract_var(dataset=dm1,var=obj,indices=indices_store) for obj in (list(dm1.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallpvarout = [largearea_withpres(dm1,mypvar[i],indices_store) for i in (range(len(mypvar)))]    \n",
    "    pvarname1 = ['outvort','outrhum']\n",
    "    pvarname = ['vort','rhum']\n",
    "    pvardict = {varnameobj:varobj for (varnameobj,varobj) in zip(pvarname,smallpvarout)}\n",
    "    \n",
    "    lonselect = dm1.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm1.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "    mymask = createmask(dm=dm1,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    ts_pdict = {}\n",
    "    for ind,obj in (enumerate(pvarname)):\n",
    "        pvarTS_store = []\n",
    "        for plevv in range(len(dm1.plev.data)):\n",
    "            tempvar = pvardict[pvarname[ind]][:,plevv,...]\n",
    "            tempts = [tempvar[i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "            tempTSERIES = [np.nanmean(obj) for obj in tempts]\n",
    "            pvarTS_store.append(tempTSERIES)\n",
    "        ts_pdict[pvarname1[ind]] = np.asarray(pvarTS_store).transpose()\n",
    "    #############################################################################################\n",
    "    TCp_ts.append(ts_pdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm1=xr.open_dataset(datapath+'/gpot/gpot_2010.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e75a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pressure vars - geopotential\n",
    "TCgp_ts = []\n",
    "for TCobj in tqdm(tracklist):\n",
    "    track=pd.read_csv(TCobj,delimiter=r\",\")\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mypvar = [extract_var(dataset=dm1,var=obj,indices=indices_store) for obj in (list(dm1.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallpvarout = [largearea_withpres(dm1,mypvar[i],indices_store) for i in (range(len(mypvar)))]    \n",
    "    pvarname1 = ['outgpot']\n",
    "    pvarname = ['gpot']\n",
    "    pvardict = {varnameobj:varobj for (varnameobj,varobj) in zip(pvarname,smallpvarout)}\n",
    "    \n",
    "    lonselect = dm1.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm1.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "    mymask = createmask(dm=dm1,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    ts_pdict = {}\n",
    "    for ind,obj in (enumerate(pvarname)):\n",
    "        pvarTS_store = []\n",
    "        for plevv in range(len(dm1.plev.data)):\n",
    "            tempvar = pvardict[pvarname[ind]][:,plevv,...]\n",
    "            tempts = [tempvar[i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "            tempTSERIES = [np.nanmean(obj) for obj in tempts]\n",
    "            pvarTS_store.append(tempTSERIES)\n",
    "        ts_pdict[pvarname1[ind]] = np.asarray(pvarTS_store).transpose()\n",
    "    #############################################################################################\n",
    "    TCgp_ts.append(ts_pdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee3b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm1=xr.open_dataset(datapath+'/vvel/w_wnd_2010.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b092fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pressure vars - vertical velocity\n",
    "TCvv_ts = []\n",
    "for TCobj in tqdm(tracklist):\n",
    "    track=pd.read_csv(TCobj,delimiter=r\",\")\n",
    "    lon1=track['lon'].to_numpy()\n",
    "    lat1=track['lat'].to_numpy()\n",
    "    pos = arr = np.stack((lat1, lon1), axis=1)\n",
    "    ###########################################################################\n",
    "    indices_store = output_indices(track,era5_date,era5_hour)\n",
    "    ###########################################################################\n",
    "    mypvar = [extract_var(dataset=dm1,var=obj,indices=indices_store) for obj in (list(dm1.keys()))]\n",
    "    ###########################################################################\n",
    "    tc_irad=np.empty((len(indices_store),4))\n",
    "    tc_irad[:,0] = pos[:,0]-2\n",
    "    tc_irad[:,1] = pos[:,0]+2\n",
    "    tc_irad[:,2] = pos[:,1]-2\n",
    "    tc_irad[:,3] = pos[:,1]+2\n",
    "    \n",
    "    tc_orad=np.empty((len(indices_store),4))\n",
    "    tc_orad[:,0] = pos[:,0]-8\n",
    "    tc_orad[:,1] = pos[:,0]+8\n",
    "    tc_orad[:,2] = pos[:,1]-8\n",
    "    tc_orad[:,3] = pos[:,1]+8\n",
    "    ###########################################################################\n",
    "    smallpvarout = [largearea_withpres(dm1,mypvar[i],indices_store) for i in (range(len(mypvar)))]    \n",
    "    pvarname1 = ['outvvel']\n",
    "    pvarname = ['vvel']\n",
    "    pvardict = {varnameobj:varobj for (varnameobj,varobj) in zip(pvarname,smallpvarout)}\n",
    "    \n",
    "    lonselect = dm1.lon.sel(lon=slice(tc_orad[0,:][2],tc_orad[0,:][3])).data\n",
    "    latselect = np.flipud(dm1.lat.sel(lat=slice(tc_orad[0,:][1],tc_orad[0,:][0])).data)\n",
    "    lon2d,lat2d = np.meshgrid(lonselect,latselect)\n",
    "    mymask = createmask(dm=dm1,irad=tc_irad,orad=tc_orad,lonselect=lonselect,latselect=latselect)\n",
    "    #############################################################################################\n",
    "    ts_pdict = {}\n",
    "    for ind,obj in (enumerate(pvarname)):\n",
    "        pvarTS_store = []\n",
    "        for plevv in range(len(dm1.plev.data)):\n",
    "            tempvar = pvardict[pvarname[ind]][:,plevv,...]\n",
    "            tempts = [tempvar[i,...][~mymask[i]] for i in range(len(mymask))]\n",
    "            tempTSERIES = [np.nanmean(obj) for obj in tempts]\n",
    "            pvarTS_store.append(tempTSERIES)\n",
    "        ts_pdict[pvarname1[ind]] = np.asarray(pvarTS_store).transpose()\n",
    "    #############################################################################################\n",
    "    TCvv_ts.append(ts_pdict)\n",
    "#myvar = [extract_var(var=obj,indices=indices_store) for obj in vars_dm1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9324e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "preslv = [str(int(obj)) for obj in dm1.plev.data/100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e235df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx=xr.open_dataset(datapath+'/tlevs/tlevs_2010.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plev=[str(int(obj)) for obj in dx.plev.data/100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023930d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storms 1\n",
    "mainkeila=pd.read_csv(output+'inner/inner_nio_era5_keila.csv',delimiter=r\",\")\n",
    "mainkeila.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "mainkeila=mainkeila.drop('a', axis=1)\n",
    "\n",
    "#pdf_keila=pd.read_csv(target+'2011_pmin_nio_keila.csv',delimiter=r\",\")\n",
    "#pdf_keila.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "#pdf_keila=pdf_keila.drop('a', axis=1)\n",
    "\n",
    "#extra=pd.read_csv(output+'nio/outder_2011_nio_keila.csv',delimiter=r\",\")\n",
    "#extra.rename({\"Unnamed: 0\":\"a\"}, axis=\"columns\", inplace=True)\n",
    "#extra=extra.drop('a', axis=1)\n",
    "\n",
    "\n",
    "slev_keila=pd.DataFrame.from_dict(TCs_ts[0])\n",
    "rlev_keila=pd.DataFrame.from_dict(TCr_ts[0])\n",
    "prlev_keila=pd.DataFrame.from_dict(TCpr_ts[0])\n",
    "prlev_keila['outmslp']=prlev_keila['outmslp']/100\n",
    "\n",
    "#ts1_keila=pd.concat([mainkeila,extra,slev_keila,rlev_keila,prlev_keila], axis=1, join='inner')\n",
    "ts1_keila=pd.concat([mainkeila,slev_keila,rlev_keila,prlev_keila], axis=1, join='inner')\n",
    "\n",
    "tempvortdict = {'outvort_'+plev[i]:TCp_ts[0]['outvort'][:,i] for i in range(21)}\n",
    "temprhumdict = {'outrhum_'+plev[i]:TCp_ts[0]['outrhum'][:,i] for i in range(21)}\n",
    "tempgpotdict = {'outgpot_'+plev[i]:TCgp_ts[0]['outgpot'][:,i] for i in range(21)}\n",
    "tempvveldict = {'outvvel_'+preslv[i]:TCvv_ts[0]['outvvel'][:,i] for i in range(18)}\n",
    "dict1 = {**tempvortdict,**temprhumdict}\n",
    "dict3 = {**tempgpotdict,**tempvveldict}\n",
    "\n",
    "alldict1 = {**dict1,**dict3}\n",
    "\n",
    "plev_keila=pd.DataFrame.from_dict(alldict1)\n",
    "\n",
    "#ts_keila=pd.concat([pdf_keila,ts1_keila,plev_keila], axis=1, join='inner')\n",
    "ts_keila=pd.concat([ts1_keila,plev_keila], axis=1, join='inner')\n",
    "ts_keila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f301204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=ts_keila\n",
    "df1=df1.drop('ws10', axis=1)\n",
    "df1=df1.drop('u10', axis=1)\n",
    "df1=df1.drop('v10', axis=1)\n",
    "df1=df1.drop('out_ws10', axis=1)\n",
    "df1=df1.drop('outu10', axis=1)\n",
    "df1=df1.drop('outv10', axis=1)\n",
    "df1=df1.drop('vi_div_ke_flux', axis=1)\n",
    "df1=df1.drop('vi_div_gpot_flux', axis=1)\n",
    "df1=df1.drop('vi_ke', axis=1)\n",
    "df1=df1.drop('vi_div_olr_flux', axis=1)\n",
    "df1=df1.drop('vi_temp', axis=1)\n",
    "df1=df1.drop('inst_10m_wnd_gst', axis=1)\n",
    "df1=df1.drop('surfmean_swr_flux', axis=1)\n",
    "df1=df1.drop('surfmean_lwr_flux', axis=1)\n",
    "df1=df1.drop('dwnwrdmean_swr_flux', axis=1)\n",
    "df1=df1.drop('topmean_lwr_flux', axis=1)\n",
    "df1=df1.drop('topmean_swr_flux', axis=1)\n",
    "df1=df1.drop('outvi_div_ke_flux', axis=1)\n",
    "df1=df1.drop('outvi_div_gpot_flux', axis=1)\n",
    "df1=df1.drop('outvi_ke', axis=1)\n",
    "df1=df1.drop('outvi_div_olr_flux', axis=1)\n",
    "df1=df1.drop('outvi_temp', axis=1)\n",
    "df1=df1.drop('outinst_10m_wnd_gst', axis=1)\n",
    "df1=df1.drop('outsurfmean_swr_flux', axis=1)\n",
    "df1=df1.drop('outsurfmean_lwr_flux', axis=1)\n",
    "df1=df1.drop('outdwnwrdmean_swr_flux', axis=1)\n",
    "df1=df1.drop('outtopmean_lwr_flux', axis=1)\n",
    "df1=df1.drop('outtopmean_swr_flux', axis=1)\n",
    "df1=df1.drop('outmslp', axis=1)\n",
    "df1=df1.drop('mslp', axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(output+'timeseries_era5_nio_keila.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
